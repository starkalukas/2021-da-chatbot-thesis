\section{Maschinelles Lernen}
\setauthor{Lukas Starka}


\section{Deep Learning}
\setauthor{Lukas Starka}


\section{Neuronale Netze}
\setauthor{Lukas Starka}
\subsection{Convolutional Neural Networks}
\setauthor{Lukas Starka}

\subsection{Recurrent Neural Networks}
\setauthor{Lukas Starka}


\section{Word Vectors}
\setauthor{Lukas Starka}


\section{Text Analysis}
\setauthor{Lukas Starka}

\subsection{Text analysis vs. Text Mining vs. Text Analytics}
\setauthor{Lukas Starka}

Meistens werden die Begriffe Text Analysis und Text Mining im selben Zusammenhang verwendet.
Dabei bedeuten sie eigentlich dasselbe, nämlich, dass man den Sinn einer Nachricht extrahiert.
Deswegen wird in den folgenden Kapiteln nur von Text Analysis gesprochen.

Es gibt jedoch einen Unterschied zwischen Text Analysis und Text Analytics.
Grundsätzlich kann man dabei sagen, dass Text Analysis qualitative Ergebnisse liefert, wohingegen bei Text Analytics die Quantität mehr im Vordergrund steht.\cite{textAnalysisMonkeylearn, machineLearningTextAnalysis}

Bei Text Analysis werden also wichtige Informationen aus der Nachricht herausgelesen.
Oder anders formuliert geht es darum, dass trotz der Vielseitigkeiten der menschlichen Sprache trotzdem die Kernaussage herausgefunden werden kann, mit der dann gearbeitet wird.
Es können also dann Informationen herausgefiltert werden, wie beispielsweise, ob etwas positiv oder negativ ist oder was das Hauptthema des Textes ist.\cite{textAnalysisMonkeylearn, machineLearningTextAnalysis}

Auf der anderen Seite wird bei Text Analytics werden verschiedene Muster aus einer großen Menge an Nachrichten herausgefiltert, welche dann in Graphen, Tabellen oder Berichten gezeigt werden können.
Bei Text Analytics geht es also darum Muster und Entwicklungen von numerischen Ergebnissen herauszufinden, bei denen dann Informationen herausgefiltert werden können, wie beispielsweise, die Prozentzahl der positiven Bewertungen.\cite{textAnalysisMonkeylearn, machineLearningTextAnalysis}

\subsection{Warum Text Analysis?}
\setauthor{Lukas Starka}

Im Gegensatz zum manuellen Aufbereiten von Texten sorgt Machine Learning für eine schnelle Bearbeitung, die außerdem auch kostengünstiger ist, weil viele Stellen wegfallen, die benötigt werden würden, um die Texte selber aufzubereiten.
Durch die Verwendung von Text Analysis spart man sich viele Arbeitskräfte, die bei anderen wichtigen Aufgaben innerhalb eines Unternehmens eingesetzt werden können.
Außerdem können dadurch Texte rund um die Uhr und zur Echtzeit bearbeitet werden.
Durch Algorithmen werden außerdem Fehler reduziert, die bei manuellem Bearbeiten leicht auftreten können und Daten können genauer aufbereitet werden.\cite{textAnalysisMonkeylearn}

\subsection{Machine Learning mit Text Analysis}
\setauthor{Lukas Starka}

Grob kann man behaupten, dass ein Text Analysis Tool aus drei verschiedenen Schritten besteht.

\begin{enumerate}
    \item Zunächst muss man sich überlegen, welche Daten gesammelt werden sollen, um damit sein Modell zu trainieren und zu testen.
    Man unterscheidet hierbei zwischen "Internal Data" und "External Data".
    Unter External Data werden Quellen, wie Zeitungen oder Foren bezeichnet und zu der Internal Data zählen sämtliche Daten, die eine Firma jeden Tag generiert, wie E-Mails, Reports, Chats oder Umfragen.
    \item Danach müssen die Daten vorbereitet werden, damit das Programm diese versteht.
    Dieser Schritt wird meistens als "Data Preprocessing" bezeichnet.
    \item Zum Schluss wird dann ein Machine Learning Algorithmus hinzugefügt, welcher sich um die Analyse kümmert.
    Diesen kann man entweder komplett selber implementieren oder man nimmt sich Libraries zur Hilfe.\cite{machineLearningTextAnalysis}
\end{enumerate}

\section{Natural Language Processing (NLP)}
\setauthor{Lukas Starka}

Natural Language Processing, oft auch als Akronym \textbf{NLP} abgekürzt, sorgt dafür, dass Computer Text auf dieselbe Art und Weise verstehen, wie wir es als Menschen tun.
NLP vereint dabei die Modellierung der menschlichen Sprache mit statistischen Machine Learning und Deep Learning Modellen.
Dies ermöglicht der Maschine im Endeffekt, dass diese menschliche Sprache in Form von Text- oder Sprachdaten zu verarbeiten und sozusagen die komplette Bedeutung und Absicht der Benutzerin oder des Benutzers zu verstehen.\cite{naturalLanguageProcessingIBM}

\subsection{Corpus}
\setauthor{Lukas Starka}

Unter dem Corpus werden die Daten bezeichnet, die verwendet werden, um das NLP Modell zu trainieren, damit dieses menschliche Sprache versteht und damit arbeiten kann.
Der Textkorpus ist also eine Menge von strukturierten Texten, die für den Computer lesbar sind.\cite{corpus}


\subsection{Tokenization}
\setauthor{Lukas Starka}

Bei der Tokenization repräsentiert jeder Token eine sinnvolle Einheit.
Darunter versteht man Wörter, Zeichen oder Sonderzeichen, die alle einen eigenen Token darstellen, lediglich Leerzeichen in einem Satz stellen keine eigenen Token dar.\cite{machineLearningTextAnalysis, naturalLanguageProcessing}

Die verschiedenen Token, die dabei entstehen, können dabei wie folgt aussehen:

\begin{lstlisting}[label={lst: Tokenization}]
Let us go to the park.

0: Let
1: us
2: go
3: to
4: the
5: park
6: .
\end{lstlisting}

Die Tokenization ist dabei besonders für verschiedene Sprachen von Vorteil, da es in diesen immer andere Regeln gibt, wie die Wörter aufgeteilt werden können.
In dem obigen Beispiel wirkt es womöglich no so, als könnte man mit einer \textbf{split()} Methode das selbe Resultat erzielen, aber es gibt auch Besonderheiten einer Sprache, die vom Tokenizer bedacht werden müssen, wie folgendes Beispiel aufzeigt.\cite{machineLearningTextAnalysis, naturalLanguageProcessing}

\begin{lstlisting}[label={lst: Tokenization Ausnahme}]
"Let's go to the U.K.!"

0: "
1: Let
2: 's
3: go
4: to
5: the
6: U.K.
7: !
8: "
\end{lstlisting}

\subsection{Part-of-speech Tagging}
\setauthor{Lukas Starka}


\subsection{Named Entity Recognition Tagging}
\setauthor{Lukas Starka}


\subsection{Stemming}
\setauthor{Lukas Starka}


\subsection{Lemmatization}
\setauthor{Lukas Starka}


\subsection{Parsing}
\setauthor{Lukas Starka}


\subsubsection{Dependency Parsing}
\setauthor{Lukas Starka}


\subsubsection{Constituency Parsing}
\setauthor{Lukas Starka}


\subsection{Stopwords}
\setauthor{Lukas Starka}


\subsection{Vectorization}
\setauthor{Lukas Starka}


\subsection{Skalarprodukt}
\setauthor{Lukas Starka}

\subsection{Kreuzprodukt}
\setauthor{Lukas Starka}


\subsection{Bag of Words}
\setauthor{Lukas Starka}


\subsection{Bag of n-grams}
\setauthor{Lukas Starka}


\subsection{Vector Spaces}
\setauthor{Lukas Starka}


\subsection{Term Frequency times Inverse Document Frequency (TF-IDF)}
\setauthor{Lukas Starka}


\subsection{Zipfsches Gesetz}
\setauthor{Lukas Starka}


\subsection{Word2Vec}
\setauthor{Lukas Starka}


\subsection{NLP und NLU Tools}
\setauthor{Lukas Starka}


\subsection{NLP in Rasa}
\setauthor{Lukas Starka}

In Rasa kann man sich die verschiedenen Schritte vom Natural Language Processing ansehen, indem man sich die \hyperref[sec:pipeline]{Pipeline} ansieht.
Diese befindet sich in der \textbf{config.yml} Datei und dort sind alle Komponenten aufgelistet, die Rasa verwendet und man kann diese nach Belieben auch selbst bearbeiten.